{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProjectML20\n",
    "by Ryan Park and David Li\n",
    "Created from scratch except for first cell\n",
    "training data set:\n",
    "https://www.kaggle.com/ngbolin/mnist-dataset-digit-recognizer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abstract:\n",
    "the purpose of this project was to create a machine learning program using the KNeighborsClassifier to precisely predict the pixels of a 28x28 .csv panda created image of one of the numbers 0-9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Product Results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have acheived a final accuracy of 98.76%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "our KNeighborsClassifier paramateers results\n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
    "                     weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is our confusion matrix:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[ 974,    1,    0,    0,    0,    0,    4,    1,    0,    0],\n",
    "       [   0, 1135,    0,    0,    0,    0,    0,    0,    0,    0],\n",
    "       [   4,    4, 1016,    0,    0,    0,    1,    7,    0,    0],\n",
    "       [   0,    0,    2,  995,    1,    6,    0,    5,    0,    1],\n",
    "       [   0,    2,    0,    0,  969,    0,    1,    2,    1,    7],\n",
    "       [   3,    0,    0,    4,    2,  881,    0,    1,    0,    1],\n",
    "       [   1,    4,    0,    0,    0,    1,  952,    0,    0,    0],\n",
    "       [   0,    7,    1,    0,    1,    0,    0, 1013,    0,    6],\n",
    "       [   2,    2,    2,    5,    1,    6,    1,    3,  949,    3],\n",
    "       [   1,    3,    2,    2,    2,    0,    1,    6,    0,  992]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP bring in the new datasets\n",
    "##DL creating variable and setting it so that it is defined as reading the panda csv file called test.csv\n",
    "test_data = pd.read_csv('test.csv')\n",
    "##DL creating variable and setting it so that it is defined as reading the panda csv file called train.csv\n",
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RP All the datasets with \"1\" afterwards are new datasets, so the ml program is trained on different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP split into traning data, traning labels, and test values\n",
    "##DL the y_train1 variable is used to train all values that are under the column label in the panda\n",
    "y_train1 = train_data['label'].values\n",
    "##DL the x_train1 is all of the pixels in the columns that are not under the label column\n",
    "X_train1 = train_data.drop(columns=['label']).values\n",
    "##DL the test.csv file does not have any labeling/first row that contains name for all the pixels\n",
    "X_test1 = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RP Book's code, here for my reference\n",
    "##RP but basically imports their own dataset, assigns the x and y strings to integers, and splits the group of numbers up into testing and training\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(np.uint8)\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DL the Kneighbors classifier analayzes based on previous binary classified numbers. \n",
    "#DL n_neighbors decides how many numbers to compare the current row of pixels to\n",
    "#DL we set a parameter of euclidean distance to be more important because we care about how the number looks on the screen\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n",
    "\n",
    "#DL the code that actually trains the program\n",
    "knn_clf.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DL determines accuracy of our program using sklearn premade accuracy function using the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_knn_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_knn_pred)\n",
    "#RP our accuracy calculated was 98.76% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RP The training dataset comes from a competition, so the testing set's labels are not included. The testing set we used\n",
    "is HOML's testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DL creating a cross validation score using 3 cross folds\n",
    "#RP note: takes a while to load\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(knn_clf, X_test, y_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 974,    1,    0,    0,    0,    0,    4,    1,    0,    0],\n",
       "       [   0, 1135,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   4,    4, 1016,    0,    0,    0,    1,    7,    0,    0],\n",
       "       [   0,    0,    2,  995,    1,    6,    0,    5,    0,    1],\n",
       "       [   0,    2,    0,    0,  969,    0,    1,    2,    1,    7],\n",
       "       [   3,    0,    0,    4,    2,  881,    0,    1,    0,    1],\n",
       "       [   1,    4,    0,    0,    0,    1,  952,    0,    0,    0],\n",
       "       [   0,    7,    1,    0,    1,    0,    0, 1013,    0,    6],\n",
       "       [   2,    2,    2,    5,    1,    6,    1,    3,  949,    3],\n",
       "       [   1,    3,    2,    2,    2,    0,    1,    6,    0,  992]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DL import sklearn's confusion matrix and use the y_test/y_knn_pred to graph \n",
    "#DL rows represent actual classes\n",
    "#DL columns represent predicted classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0b3cde2eeed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_knn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_knn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1253\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "#DL precision/recall does not work in our case because we did not create a binary classifier. \n",
    "#RP Function does not take multiclass classifier.\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_test,y_knn_pred)\n",
    "recall_score(y_test, y_knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
